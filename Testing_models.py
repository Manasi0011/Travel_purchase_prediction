# -*- coding: utf-8 -*-
"""Tourism_Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CwUzVxnaaJvpyhTxCifnNM5pjlqb5kJ7
"""

# Data manipulation and handling libraries
import numpy as np
import pandas as pd

# Data visualization libraries
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.stats as sci

# Warning suppression
import warnings

warnings.filterwarnings

df = pd.read_csv("Tourist_Dataset.csv")

df.head()

"""**Data Dictionary**

* CustomerID: Unique customer ID
* ProdTaken: Whether the customer has purchased a package or not (0: No, 1: Yes)
* Age: Age of customer
* TypeofContact: How customer was contacted (Company Invited or Self Inquiry)
* CityTier: City tier depends on the development of a city, population, facilities, and living standards. The categories are ordered i.e. Tier 1 > Tier 2 > Tier 3
* Occupation: Occupation of customer
* Gender: Gender of customer
* NumberOfPersonVisiting: Total number of persons planning to take the trip with the customer
* PreferredPropertyStar: Preferred hotel property rating by customer
* MaritalStatus: Marital status of customer
* NumberOfTrips: Average number of trips in a year by customer
* Passport: The customer has a passport or not (0: No, 1: Yes)
* OwnCar: Whether the customers own a car or not (0: No, 1: Yes)
* NumberOfChildrenVisiting: Total number of children with age less than 5
planning to take the trip with the customer
* Designation: Designation of the customer in the current organization
* MonthlyIncome: Gross monthly income of the customer
* Customer interaction data:
* PitchSatisfactionScore: Sales pitch satisfaction score
* ProductPitched: Product pitched by the salesperson
* NumberOfFollowups: Total number of follow-ups has been done by the salesperson after the sales pitch
* DurationOfPitch: Duration of the pitch by a salesperson to the customer


"""

df.info()

"""Observations here:
* ProdTaken is our dependent variable and needs to be converted to categorical variable
* There are some null values
* 4888 entries 20 columns
* More insights on data are required

### **Defining Target Feature**
"""

Y = df["ProdTaken"]
X = df.drop("ProdTaken", axis=1)

"""### **Data Preprocessing**"""

# Firstly checking for duplicated data
df.duplicated().sum()

"""Since there are no duplicates we can move ahed to segregate the data between
numerical and categorical and working on data cleaning

### **Segregating numerical and categorical**
"""

df_num = df.select_dtypes(['int64', 'float64'])
df_num.head().transpose()

# ProdTaken, CityTier, NumberOfPersonVisited, NumberOfFollowups, PreferredPropertyStar,
# NumberOfTrips, Passport, PitchSatisfactionScore, OwnCar, NumberOfChildrenVisited

# Only Age and monthly income are the numerical features
# So we will change their data types


convert_typelist = ['ProdTaken', 'CityTier', 'NumberOfPersonVisited', 'NumberOfFollowups', 'PreferredPropertyStar',
                    'NumberOfTrips', 'Passport', 'PitchSatisfactionScore', 'OwnCar', 'NumberOfChildrenVisited']

for col in convert_typelist:
    df[col] = df[col].astype('object')

df_cat = df.select_dtypes(['object'])
df_cat.head()

cols = df.select_dtypes(["object"])

for i in cols.columns:
    df[i] = df[i].astype("category")
    df_cat[i] = df_cat[i].astype("category")

df_num = df.select_dtypes(['int64', 'float64'])
df_num.head()

"""## **Treating the missing values first**"""

df_num.isnull().sum()

df_cat.isnull().sum()

"""Age and Monthly income are related to Designation. And Designation does not have missing values. So we can use that column to fill in these values.

Duration of pitch can be populated by the median values.

NumberOfFollowups, PreferredPropertyStar, NumberOfTrips, NumberOfChildrenVisited, can be populated by the median values   

TypeofContact
"""

# filling the median value

for col in df_num.columns:
    df[col] = df_num[col].fillna(df[col].median())

# filling up mode in the categorical columns
for col in df_cat.columns:
    df[col] = df_cat[col].fillna(df[col].value_counts().index[0])

df_cat.head()

df.isnull().sum()

df_num = df.select_dtypes(['int64', 'float64'])

"""### **Looking through Categorical columns**"""

# get the valuecounts
for i in df_cat.columns:
    print(df_cat[i].value_counts())
    print("-" * 50)
    print("\n")

"""### **Cleaning the strings**

* In the Gender column we have a third category as "Fe Male". Which we'll treat as a data entry issue.
* Married people take the most trips
* Self Enquiry is the most preffered in TypeofContact feature.
* 3.0 is the highest property preferred
* Majority people prefer their own car
* The basic package is the highest sold
"""

# Cleaning the Gender col string
# treating error
df_cat.Gender = df.Gender.replace("Fe Male", "Female")

# verify the update
df_cat.Gender.value_counts()

"""Number of trips has too many categories and we shall treat it as numerical feature

Along with Duration of Pitch

## **Descriptive Analysis**

### **Functions**
"""


def histogram_boxplot(data, feature, figsize=(12, 7), kde=False, bins=None):
    f2, (ax_box2, ax_hist2) = plt.subplots(nrows=2,  # Number of rows of the subplot grid= 2
                                           sharex=True,  # x-axis will be shared among all subplots
                                           gridspec_kw={"height_ratios": (0.25, 0.75)},
                                           figsize=figsize)  # creating the 2 subplots
    quantiles = np.array([df[feature].quantile(0.00),
                          df[feature].quantile(0.25),
                          df[feature].quantile(0.50),
                          df[feature].quantile(0.75),
                          df[feature].quantile(1.00)])

    sns.boxplot(data=data, x=feature, ax=ax_box2, showmeans=True, color="orange")
    # boxplot will be created and a marker will indicate the mean value of the column
    # with showmenas = True
    ax_box2.vlines(quantiles, [0] * quantiles.size, [1] * quantiles.size,
                   color='b', ls=':', lw=0.5, zorder=0)
    ax_box2.set_xticks(quantiles)
    ax_box2.set_xlabel('value')

    if bins:
        sns.histplot(data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins, palette="Winter")
    else:
        sns.histplot(data=data, x=feature, kde=kde, ax=ax_hist2)  # For histogram

    ax_hist2.axvline(data[feature].mean(), color="green", linestyle="--")  # Green line shows mean
    ax_hist2.axvline(data[feature].median(), color="blue", linestyle="-")  # blue line shows median
    plt.show()  # show the plot


# labeled_barplot
def labeled_barplot(data, feature, perc=False, v_ticks=True, n=None):
    # Barplot with percentage at the top

    total = len(data[feature])  # length of the column
    count = data[feature].nunique()
    if n is None:
        plt.figure(figsize=(count + 1, 5))
    else:
        plt.figure(figsize=(n + 1, 5))

    # if v_ticks is True:
    #     plt.xticks(rotation=90)

    ax = sns.countplot(data=data,
                       x=feature,
                       palette="muted",
                       order=data[feature].value_counts().index[:n].sort_values())

    for p in ax.patches:  # Every Bar in the chart is called a patch
        if perc == True:
            label = "{:.1f}%".format(100 * p.get_height() / total)
            # percentage of each class of the category
        else:
            label = p.get_height()  # count of each level of the category

        x = p.get_x() + p.get_width() / 2  # width of the plot
        y = p.get_height()  # height of the plot

        ax.annotate(label, (x, y),
                    ha="center",
                    va="center",
                    size=12,
                    xytext=(0, 5),
                    textcoords="offset points")  # annotate the percentage
    plt.show()  # show the plot


def stacked_barplot(data, predictor, target):
    """
    Print the category counts and plot a stacked bar chart
    """
    count = data[predictor].nunique()
    sorter = data[target].value_counts().index[-1]
    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(by=sorter, ascending=False)
    print(tab1)
    print("-" * 120)
    tab = pd.crosstab(data[predictor], data[target], normalize="index").sort_values(by=sorter, ascending=False)

    tab.plot(kind="bar", stacked=True, figsize=(count + 5, 6))
    plt.legend(loc="lower left", frameon=False)
    plt.legend(loc="upper left", bbox_to_anchor=(1, 1))

    plt.show()


def perc_stacked_barplot(df, x_var, y_var):
    count = df[x_var].nunique()
    sorter = df[y_var].value_counts().index[-1]
    tab1 = pd.crosstab(df[x_var], df[y_var], margins=True).sort_values(by=sorter, ascending=False)
    print(tab1)
    print("-" * 120)
    tab = pd.crosstab(df[x_var], df[y_var], normalize="index").sort_values(by=sorter, ascending=False)

    df_grouped = df.groupby(x_var)[y_var].value_counts(normalize=True).unstack(y_var)
    df_grouped.plot.bar(stacked=True)
    plt.legend(bbox_to_anchor=(0.5, 1.02),
               loc="lower center",
               borderaxespad=0,
               frameon=False,
               ncol=3)
    for ix, row in df_grouped.reset_index(drop=True).iterrows():
        cumulative = 0
        for element in row:
            if element > 0.1:
                plt.text(ix,
                         cumulative + element / 2,
                         f"{int(element * 100)} %",
                         va="center",
                         ha="center")
            cumulative += element
    plt.tight_layout()


"""### **Target Variable - Product Taken**"""

# use label_barplot function to plot the graph
labeled_barplot(df_cat, "ProdTaken", True, False)

"""* Only 18.8% of the customers actually buy the product
* There is a high imbalance in class

### **Summary of numerical columns**
"""

df_num = df.select_dtypes(["int64", "float64"])
df_num.describe()

""" Customer ID """
# dropping the customer id here since we would not need it for prediction purposes
df_num = df_num.drop('CustomerID', axis=1)
df = df.drop('CustomerID', axis=1)

""" Age """
# looking at the distribution first
histogram_boxplot(df_num, "Age")

# Checking skewness
df_num["Age"].skew()

Q1 = df_num["Age"].quantile(0.25)
Q3 = df_num["Age"].quantile(0.75)
print("Middle 50 % data falls in ", Q1, "and", Q3)

"""* Age does follow a mostly normal distribution and is slightly positively skewed.
* There are no outliers
* Most customers are in the age bracket between 31 and 43
"""

""" Duration of Pitch """
# looking at the distribution first
histogram_boxplot(df_num, "DurationOfPitch")

"""* There are some outlliers present that is affecting the mean n median values.
* Most of the customers had the pitch under 20 mins but some are seen taking upto 40 mins and some even upto 120mins
"""

""" Monthly Income """
# looking at the distribution first
histogram_boxplot(df_num, "MonthlyIncome")

"""* MonthlyIncome is right-skewd.
* However, we see that the majority of customers are between income bracket 20K dollars and 30K dollars.
* We also see two outliers in the low end and on the highest end.
* There are several outliers after the approx 35K dollars income level.

### **Categorical data**
"""

""" TypeofContact """
labeled_barplot(df_cat, "TypeofContact", True, False)

"""Self enquiry is the most preffered type of contact"""

""" City Tier """
labeled_barplot(df_cat, "CityTier", True, False)

"""Most of the customers are from Tier 1 cities (65%) and next comes tier3 cities"""

""" Occupation """
labeled_barplot(df_cat, "Occupation", True, False)

"""Salaried customers are the highest, then the small businessess. The large business owners are only 9 %"""

""" Gender """
labeled_barplot(df_cat, "Gender", True, False)

"""Male customers are more than the number of female customers"""

""" NumberOfPersonVisited """
labeled_barplot(df_cat, "NumberOfPersonVisited", True, False)

"""Majority of the customers have 3 people with them (probably famalies), there are also customers having 2 and 3 people, .8% travel in couples.
.1% have 5 people with them
"""

""" NumberOfFollowups """
labeled_barplot(df_cat, "NumberOfFollowups", True, False)

"""Almost 40% people require 4 followups and 30% require 3 followups, there are a few with 5 and 6 followups"""

""" ProductPitched """
labeled_barplot(df_cat, "ProductPitched", True, False)

"""Most people look for the Basic and Deluxe package"""

""" PreferredPropertyStar """
labeled_barplot(df_cat, "PreferredPropertyStar", True, False)

"""61% prefer 3 star property"""

""" MaritalStatus """
labeled_barplot(df_cat, "MaritalStatus", True, False)

"""Almost half of the customers are Married"""

""" Number of trips """
labeled_barplot(df_cat, "NumberOfTrips", True, False)

"""Average number of trips in a year by customer 50 % have 2 - 3 trips
* The distribution is slightly right skewed, and  there can be seen some outliers
"""

# """ Duration of Pitch """
# labeled_barplot(df_cat,"DurationOfPitch", True, False)

""" Passports """
labeled_barplot(df_cat, "Passport", True, False)

"""70% customers already posess a passport"""

""" PitchSatisfactionScore """
labeled_barplot(df_cat, "PitchSatisfactionScore", True, False)

"""* Only 30.2% of customers rated the Sales Pitch with a score of 3.
* Even though 18.7% customers rated at 4.
* 19.8% rated a pitch score of 5.
* we also see that 19.3% rated the Sales pitch score at 1.
* This shows a need for improvement in this area.
"""

""" Own Car """
labeled_barplot(df_cat, "OwnCar", True, False)

"""60% do posess their own car, since this shows enough variation the feature can't be dropped"""

""" NumberofChildrenVisited """
labeled_barplot(df_cat, "NumberOfChildrenVisited", True, False)

"""Most people have a single child with them, tho 22% do not have children with them"""

""" Designation """
labeled_barplot(df_cat, "Designation", True, False)

"""Most of our customers have Executive and Managers positions.

## **Bivariate Analysis**
Relation between numerical features and target variable
"""

# Comparision of Numerical Variables with ProdTaken to understand the relation
num_cols = df.select_dtypes(include=np.number).columns.tolist()

plt.figure(figsize=(12, 8))
for i, variable in enumerate(num_cols):
    plt.subplot(3, 2, i + 1)
    sns.boxplot(y=df["ProdTaken"], x=df[variable], palette="Set1")
    plt.tight_layout()
    plt.title(variable)
plt.show()

"""* The mean Age for customers who purchased any Product is slightly less than those who didn"t. We also see that Age variable doesn"t have any outliers.
* The mean DurationofPitch for both classed of ProdTaken is almost equal. We see there are many outliers in Class "0" of ProdTaken, suggesting that longer pitch durations doesn"t lead to product purchase.
* MonthlyIncome - the averages are almost the same. Tho there are several outliers in the higher end for both ProdTaken classes and very few in low end of Class "0".

Relation between categorical features and target variable
"""

perc_stacked_barplot(df_cat, "TypeofContact", "ProdTaken")

"""More Customers with "Company Invited" contact have bought Travel Package when compared to Customers with "Self Enquiry"."""

perc_stacked_barplot(df_cat, "CityTier", "ProdTaken")

"""More Customers from Tier 2 and 3 cities have purchased Travel Packages.


"""

perc_stacked_barplot(df_cat, "Occupation", "ProdTaken")

"""* Customers who are Freelancers by Occupation have bought travel packages. However the sample size is only two.
* Of the 434 Large Business owning customers, almost 30% bought travel packages.
* Among Salaried and Small Business owning customers,close to 20% have bought travel packages.
"""

perc_stacked_barplot(df_cat, "Gender", "ProdTaken")

"""Number of Male customers are higher than Female customers, however we dont see a lot of difference in the percentage of each Gender select the Product.


"""

perc_stacked_barplot(df_cat, "NumberOfPersonVisited", "ProdTaken")

"""* Customers who plan to take between 2-4 persons with them during travel, close to 20% have bought a travel package product.
* We see that all Customers with one companion and five companions, did not purchase any product.
* This suggests that the products don"t seem either appealing or beneficial to the customers of the above two categories.
* Business should focus on this area.
"""

perc_stacked_barplot(df_cat, "NumberOfFollowups", "ProdTaken")

"""With the increase in number of followups the percentage of customers buyingthe products is seen to increase."""

perc_stacked_barplot(df_cat, "ProductPitched", "ProdTaken")

"""* The Basic Package is the most preferred
* Standard and Deluxe are following.
* Very few customers purchased Super Deluxe products.
"""

perc_stacked_barplot(df_cat, "PreferredPropertyStar", "ProdTaken")

"""* Though majority of customers prefer a 3.0 star rated Property, the percentage of customers purchasing the products is comparatively less than customers who prefer a 4.0 and 5.0 star rated property.
* The higher the property star rating, higher the number of customers who purchased a product.
"""

perc_stacked_barplot(df_cat, "MaritalStatus", "ProdTaken")

"""* Around 30% of all Single customers have bought a product and about 25% of Unmarried customers have also purchased a product.
* Almost 50% of the total customers belong to the married category, but we see that only approx 15% of them have actually purchased any product.
"""

perc_stacked_barplot(df_cat, "Passport", "ProdTaken")

"""Customers with passport tend to purchase products than those who don"t.


"""

perc_stacked_barplot(df_cat, "PitchSatisfactionScore", "ProdTaken")

"""* Maximum customers have given the score 3 and 5.
* Majority of customers have given a score of 3.0 to the Sale pitch for the products.
* But we observe that the number of customers who purchased any product is almost equal across all pitch scores.
"""

perc_stacked_barplot(df_cat, "OwnCar", "ProdTaken")

"""Customers with own car and no car, their package purchase percentage is almost the same."""

perc_stacked_barplot(df_cat, "NumberOfChildrenVisited", "ProdTaken")

"""The number of children visiting does not affect the purchase percentage and it does not vary much"""

perc_stacked_barplot(df_cat, "Designation", "ProdTaken")

"""* Customers who are executives tend to purchase the package more.
* AVP's are least likely to purchase the package
"""

# heatmap for correlation
plt.figure(figsize=(15, 7))
colormap = sns.color_palette("Blues", 12)
sns.heatmap(df.corr(), annot=True, vmin=-1, vmax=1, fmt=".2f", cmap=colormap)
plt.show()

"""* Only Age is correlated to Monthly Income significantly,  0.47;
 i.e as Age increases, so does MontlyIncome
* Age and Duration of pitch have a slightly negative correlation

## **Treating outliers**

Only Duration of Pitch, Number of trips and Monthly Income have outliers. Since we shall experiment the data with logistic regression as well, we need to be cap the outliers.
"""


def outlier_cap(x):
    x = x.clip(lower=x.quantile(0.05))  # method assigns values outside boundary to boundary values
    x = x.clip(upper=x.quantile(
        0.95))  # The quantile() method calculates the quantile of the values in a given axis. Default axis is row.
    return (x)


for col in df_num.columns:
    df_num[col] = outlier_cap(df_num[col])

# checking the distribution now
plt.figure(figsize=(20, 30))

for i, variable in enumerate(num_cols):
    plt.subplot(5, 4, i + 1)
    plt.boxplot(df[variable], whis=1.5)
    plt.tight_layout()
    plt.title(variable)

plt.show()

df.head()

"""## **Creating Dummies**"""

Y = df_cat["ProdTaken"]
df_cat = df_cat.drop("ProdTaken", axis=1)

# creating features with n-1 variables
df_dummies = pd.get_dummies(df_cat, drop_first=True)
df_dummies.head()

"""## **Selecting KBest**"""

# selecting the Kbest for categorical cols
# chi sq test used to compare and score the categorical features

from sklearn.feature_selection import SelectKBest, chi2

selector = SelectKBest(chi2, k=20)
selector.fit_transform(df_dummies, Y)

# Get columns to keep and create a new dataframe with selected features
cols = selector.get_support(indices=True)
selected_features_df_char = df_dummies.iloc[:, cols]

X = pd.concat([selected_features_df_char, df_num], axis=1, join='inner')
X.head()

"""## **Splitting Data**"""

from sklearn.model_selection import train_test_split

# splitting data into training and test set, use stratify to maintain the original distribution of Dependent variable as of original set
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=25, stratify=Y)

# creating a list of column names
feature_names = X_train.columns.to_list()

print("Original ProdTaken True Values    : {0} ({1:0.2f}%)".format(len(df.loc[df["ProdTaken"] == 1]), (
        len(df.loc[df["ProdTaken"] == 1]) / len(df.index)) * 100))
print("Original ProdTaken False Values   : {0} ({1:0.2f}%)".format(len(df.loc[df["ProdTaken"] == 0]), (
        len(df.loc[df["ProdTaken"] == 0]) / len(df.index)) * 100))
print()
print("Training ProdTaken True Values    : {0} ({1:0.2f}%)".format(len(Y_train[Y_train[:] == 1]), (
        len(Y_train[Y_train[:] == 1]) / len(Y_train)) * 100))
print("Training ProdTaken False Values   : {0} ({1:0.2f}%)".format(len(Y_train[Y_train[:] == 0]), (
        len(Y_train[Y_train[:] == 0]) / len(Y_train)) * 100))
print()
print("Test ProdTaken True Values        : {0} ({1:0.2f}%)".format(len(Y_test[Y_test[:] == 1]),
                                                                   (len(Y_test[Y_test[:] == 1]) / len(Y_test)) * 100))
print("Test ProdTaken False Values       : {0} ({1:0.2f}%)".format(len(Y_test[Y_test[:] == 0]),
                                                                   (len(Y_test[Y_test[:] == 0]) / len(Y_test)) * 100))

"""## **Model Building**

### **Function**
"""

from sklearn import metrics

# to get diferent metric scores
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    recall_score,
    precision_score,
    confusion_matrix,
    make_scorer,
    roc_auc_score,
    precision_recall_curve,
    roc_curve,
    classification_report,
)


def get_metrics_score(model, flag=True):
    """
    model : classifier to predict values of X

    """
    # defining an empty list to store train and test results
    score_list = []

    # predicting on train and tests
    pred_train = model.predict(X_train)
    pred_test = model.predict(X_test)

    # accuracy of the model
    train_acc = model.score(X_train, Y_train)
    test_acc = model.score(X_test, Y_test)

    # recall of the model
    train_recall = metrics.recall_score(Y_train, pred_train)
    test_recall = metrics.recall_score(Y_test, pred_test)
    # precision of the model
    train_precision = metrics.precision_score(Y_train, pred_train)
    test_precision = metrics.precision_score(Y_test, pred_test)

    # f1_score of the model
    train_f1 = metrics.f1_score(Y_train, pred_train)
    test_f1 = metrics.f1_score(Y_test, pred_test)

    # populate the score_list
    score_list.extend(
        (train_acc, test_acc, train_recall, test_recall, train_precision, test_precision, train_f1, test_f1))

    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.
    if flag == True:
        print("Accuracy on training set : ", train_acc)
        print("Accuracy on test set : ", test_acc)
        print("Recall on training set : ", train_recall)
        print("Recall on test set : ", test_recall)
        print("Precision on training set : ", train_precision)
        print("Precision on test set : ", test_precision)
        print("F1 on training set : ", train_f1)
        print("F1 on test set : ", test_f1)

    return score_list  # returning the list with train n test scores


"""### **Algorithms**"""

# ML libraries
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier

# from sklearn.ensemble import RandomForestClassifier
# from sklearn.ensemble import GradientBoostingClassifier
# from sklearn.ensemble import AdaBoostClassifier
# from sklearn.svm import SVC

"""### **Logistic Regression**"""

lr = LogisticRegression()
lr.fit(X_train, Y_train)

lr_score = get_metrics_score(lr)

"""### **Decision Tree Classifier**"""

# DecistionTreeClassifier with gini and class_weight for appropriate importance
dtc = DecisionTreeClassifier(criterion="gini", class_weight={0: 0.15, 1: 0.85}, random_state=1)

# fit the model on training dataset
dtc.fit(X_train, Y_train)

# grid search CV for hyperparameter tuning of our Decision Tree
np.random.seed(44)
from sklearn.model_selection import GridSearchCV

param_dist = {'max_depth': [3, 5, 6, 7], 'min_samples_split': [50, 100, 150, 200, 250]}
tree_grid = GridSearchCV(dtc, cv=10, param_grid=param_dist, n_jobs=3)
tree_grid.fit(X_train, Y_train)
print('Best Parameters using grid search: \n', tree_grid.best_params_)

# check the scores on Training and Testing Datasets
dtc_score = get_metrics_score(dtc)

"""## **Storing model in pkl file**"""

import pickle

pickle.dump(dtc, open("model.pkl", "wb"))
model = pickle.load(open("model.pkl", "rb"))
